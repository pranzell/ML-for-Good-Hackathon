{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# README!\n- Runs contextual learning to get similar records and using this runs iterative clustering to get a final list of similar vectors.\n- Uses 'all-distilroberta-v1' for vectorization\n- Uses spacy for cleaning and lemmatization\n- Uses sparse matrix multiplication for generating similar vetors\n- Uses heusristic for number of clusters\n- **Just run the flow with your data set, keeping ::config:: updated everywhere!**","metadata":{}},{"cell_type":"markdown","source":"# File structure","metadata":{}},{"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-26T09:10:40.635332Z","iopub.execute_input":"2021-11-26T09:10:40.635685Z","iopub.status.idle":"2021-11-26T09:10:40.647439Z","shell.execute_reply.started":"2021-11-26T09:10:40.635637Z","shell.execute_reply":"2021-11-26T09:10:40.646742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"!pip install xlrd\n!pip install xlsxwriter\n!pip install ftfy\n!pip install pyspellchecker\n!pip install sparse_dot_topn==0.2.9","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:10:41.133512Z","iopub.execute_input":"2021-11-26T09:10:41.134269Z","iopub.status.idle":"2021-11-26T09:11:24.689859Z","shell.execute_reply.started":"2021-11-26T09:10:41.134226Z","shell.execute_reply":"2021-11-26T09:11:24.688612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!python/3.7.5/bin/python\n\nimport os\nimport sys\nimport json\nimport re\nimport io\nfrom io import StringIO\nimport inspect\nimport shutil\nimport ast\nimport string\nimport time\nimport pickle\nimport glob\nimport traceback\nimport multiprocessing\nimport requests\nimport logging\nimport math\nimport pytz\nfrom retrying import retry\nfrom itertools import chain\nfrom string import Template\nfrom datetime import datetime, timedelta\nfrom collections import defaultdict, OrderedDict\nfrom contextlib import contextmanager\nfrom math import pi\nimport unicodedata\nfrom collections import defaultdict, Counter\nimport ntpath\nimport tqdm\nfrom functools import reduce\nimport itertools\nfrom spellchecker import SpellChecker\n\n# graph\nimport networkx as nx\n\n# Standard\nimport numpy as np\nfrom numpy import array\nfrom numpy import argmax\nimport pandas as pd\nimport xlrd\nimport xlsxwriter\nimport jsonschema\nfrom fuzzywuzzy import fuzz\nfrom wordcloud import WordCloud\n\n# scikit-learn\nfrom sklearn.utils import shuffle\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, silhouette_score, homogeneity_score\nfrom sklearn.metrics.pairwise import euclidean_distances, cosine_similarity\nfrom sklearn.neighbors import NearestNeighbors, LocalOutlierFactor\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\n\n# scipy\nfrom scipy import spatial, sparse\nfrom scipy.sparse import coo_matrix, vstack, hstack\nfrom scipy.spatial.distance import euclidean, jensenshannon, cosine, cdist\nfrom scipy.io import mmwrite, mmread\nfrom scipy.stats import entropy\nfrom scipy.cluster.hierarchy import dendrogram, ward, fcluster\nimport scipy.cluster.hierarchy as sch\nfrom scipy.sparse.csr import csr_matrix\nfrom scipy.sparse.lil import lil_matrix\nfrom scipy.sparse.csgraph import connected_components\nfrom typing import Tuple, NamedTuple, List, Optional, Union\nfrom functools import wraps\nfrom sparse_dot_topn import awesome_cossim_topn\nimport sparse_dot_topn.sparse_dot_topn as ct\n\n# fit text for you (ftfy)\nimport ftfy\n\n# Gensim\nimport gensim\nfrom gensim.models import Phrases, Word2Vec, KeyedVectors, FastText, LdaModel\nfrom gensim import utils\nfrom gensim.utils import simple_preprocess\nfrom gensim.test.utils import datapath, get_tmpfile\nfrom gensim.scripts.glove2word2vec import glove2word2vec\nimport gensim.downloader as api\nfrom gensim import models, corpora, similarities\n\n# NLTK\nimport nltk\nfrom nltk import FreqDist\nfrom nltk import tokenize, sent_tokenize, word_tokenize, pos_tag\nfrom nltk.corpus import stopwords, PlaintextCorpusReader\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.lancaster import LancasterStemmer\nfrom nltk.stem.porter import *\nfrom nltk.translate.bleu_score import sentence_bleu\nprint(\"nltk loaded.\")\n\n# Spacy\nimport spacy\nfrom spacy import displacy\nnlp = spacy.load('en_core_web_lg')\nfrom spacy.lang.en import English\nprint(\"spacy loaded.\")\n\n# torch\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport transformers\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelWithLMHead\nfrom transformers import pipeline\nfrom transformers import AutoModel\nfrom typing import Any, Dict, List, Callable, Optional, Tuple, Union\nfrom sklearn.base import BaseEstimator, TransformerMixin\nprint(\"pyTorch loaded.\")\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:11:24.692948Z","iopub.execute_input":"2021-11-26T09:11:24.693357Z","iopub.status.idle":"2021-11-26T09:11:27.049293Z","shell.execute_reply.started":"2021-11-26T09:11:24.693309Z","shell.execute_reply":"2021-11-26T09:11:27.048498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Input Data","metadata":{}},{"cell_type":"code","source":"## Path\nresources_dir_path = \"/kaggle/input/zenbird-resources/\"","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:11:27.051208Z","iopub.execute_input":"2021-11-26T09:11:27.051552Z","iopub.status.idle":"2021-11-26T09:11:27.056594Z","shell.execute_reply.started":"2021-11-26T09:11:27.051496Z","shell.execute_reply":"2021-11-26T09:11:27.055546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## input \n\ndf_parent_org = pd.read_csv(os.path.join(resources_dir_path, \"df_parent_common.csv\"))\ndf_adult_org = pd.read_csv(os.path.join(resources_dir_path, \"df_adult_common.csv\"))\n\nprint(\"data loaded. Shapes = \", df_parent_org.shape, df_adult_org.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:11:27.059367Z","iopub.execute_input":"2021-11-26T09:11:27.059742Z","iopub.status.idle":"2021-11-26T09:11:27.092415Z","shell.execute_reply.started":"2021-11-26T09:11:27.059694Z","shell.execute_reply":"2021-11-26T09:11:27.091426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_cleaned_data(data):\n    data = data.fillna(\"<>\")\n    data['Corpus'] = data['specifypositive_April2020'] + \\\n                          \". \" + data['specifypositive_May2020'] + \\\n                          \". \" + data['specifypositive_Nov2020'] + \\\n                          \". \" + data['specifypositive_April2021']\n    data['Corpus'] = data['Corpus'].apply(lambda x: str(x).replace(\"<>.\", \"\").replace(\"<>\", \"\").strip())\n    data = data[data.Corpus.apply(lambda x: str(x).strip()==\"\") == False].reset_index(drop=True)\n    data = data.replace(\"<>.\", \"\").replace(\"<>\", \"\")\n    return data","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:11:27.093588Z","iopub.execute_input":"2021-11-26T09:11:27.094158Z","iopub.status.idle":"2021-11-26T09:11:27.101284Z","shell.execute_reply.started":"2021-11-26T09:11:27.094119Z","shell.execute_reply":"2021-11-26T09:11:27.100613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_parent = get_cleaned_data(df_parent_org)\ndf_adult = get_cleaned_data(df_adult_org)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:11:27.10225Z","iopub.execute_input":"2021-11-26T09:11:27.102977Z","iopub.status.idle":"2021-11-26T09:11:27.139959Z","shell.execute_reply.started":"2021-11-26T09:11:27.102929Z","shell.execute_reply":"2021-11-26T09:11:27.139019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_parent.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:11:27.141298Z","iopub.execute_input":"2021-11-26T09:11:27.141586Z","iopub.status.idle":"2021-11-26T09:11:27.156619Z","shell.execute_reply.started":"2021-11-26T09:11:27.141548Z","shell.execute_reply":"2021-11-26T09:11:27.155751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_adult.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:11:27.157812Z","iopub.execute_input":"2021-11-26T09:11:27.158294Z","iopub.status.idle":"2021-11-26T09:11:27.17725Z","shell.execute_reply.started":"2021-11-26T09:11:27.158257Z","shell.execute_reply":"2021-11-26T09:11:27.176621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Select a dataframe","metadata":{}},{"cell_type":"code","source":"df = df_adult.reset_index(drop=True).dropna(subset=['Corpus'])\nprint(\"Dataset length: \", df.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:49:52.027591Z","iopub.execute_input":"2021-11-26T09:49:52.029209Z","iopub.status.idle":"2021-11-26T09:49:52.050136Z","shell.execute_reply.started":"2021-11-26T09:49:52.029139Z","shell.execute_reply":"2021-11-26T09:49:52.049205Z"},"trusted":true},"execution_count":192,"outputs":[]},{"cell_type":"code","source":"## CONFIG :: column settings\n\n# chwat-history (master)\nunique_id_col = \"ID\"\ntext_col = \"Corpus\"\nclean_text_col = \"clean_text\"\nprint(\"Settings set into config.\")","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:11:27.198688Z","iopub.execute_input":"2021-11-26T09:11:27.199105Z","iopub.status.idle":"2021-11-26T09:11:27.208233Z","shell.execute_reply.started":"2021-11-26T09:11:27.199052Z","shell.execute_reply":"2021-11-26T09:11:27.207415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"# question words\nwh_words = [\"who\", \"what\", \"where\", \"when\", \"would\", \"which\", \"how\", \"why\", \"can\", \"may\", \"will\", \"won't\", \"does\", \"does not\",\n            \"doesn't\", \"do\", \"do i\", \"do you\", \"is it\", \"would you\", \"is there\", \"are there\", \"is it so\", \"is this true\" ,\n            \"to know\", \"is that true\", \"are we\", \"am i\", \"question is\", \"can i\", \"can we\", \"tell me\", \"can you explain\",\n            \"how ain't\", \"question\", \"answer\", \"questions\", \"answers\", \"ask\", \"can you tell\"]\n# greetings\ngreeting_file = os.path.join(resources_dir_path, \"/kaggle/input/zenbird-resources/greeting_words.txt\")\nwith open(greeting_file, 'r', encoding='utf-8', errors='ignore') as f:\n    greeting_words = [eval(x.rstrip())[0] for x in f.readlines()]\n\n# signature\nsignature_file = os.path.join(resources_dir_path, \"/kaggle/input/zenbird-resources/signature_words.txt\")\nwith open(signature_file, 'r', encoding='utf-8', errors='ignore') as f:\n    signature_words = [eval(x.rstrip())[0] for x in f.readlines()]\n\n# for marking keywords to retain during pre-processing step\nvocab_list = []\nvocab_list = wh_words + greeting_words + signature_words\nvocab_list = list(map(str.lower, vocab_list))\nprint(\"custom vocab loaded. total words =\", len(vocab_list))\n","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:11:27.209795Z","iopub.execute_input":"2021-11-26T09:11:27.210233Z","iopub.status.idle":"2021-11-26T09:11:27.229614Z","shell.execute_reply.started":"2021-11-26T09:11:27.210199Z","shell.execute_reply":"2021-11-26T09:11:27.228728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Preprocessing\nclass basic_preprocessing:\n    \"\"\"\n    Perform basic pre-processing steps on any textual data. To add customization, pass a list of keyowrds which needs to\n    be excluded from lemmatization or stop-word removal. Use  'vocab_list'  param to pass a customized list of keywords!\n    \"\"\"\n\n    def __init__(self, resources_dir_path, vocab_list=[], custom=False):\n        self.stopwords_file = os.path.join(resources_dir_path, \"stopwords.txt\")\n        self.special_characters_file = os.path.join(resources_dir_path, \"special_characters.txt\")\n        self.contractions_file = os.path.join(resources_dir_path, \"contractions.json\")\n        self.chatwords_file = os.path.join(resources_dir_path, \"chatwords.txt\")\n        self.emoticons_file = os.path.join(resources_dir_path, \"emoticons.json\")\n        self.greeting_file = os.path.join(resources_dir_path, \"greeting_words.txt\")\n        self.signature_file = os.path.join(resources_dir_path, \"signature_words.txt\")\n        self.vocab_list = vocab_list\n        self.custom = custom\n        self.load_resources()\n        return\n\n    def load_resources(self):\n        # stopwords, special_stopwords, special_characters\n        with open(self.stopwords_file, 'r', encoding='utf-8', errors='ignore') as f:\n            self.stopwords = [x.rstrip() for x in f.readlines()]\n        with open(self.special_characters_file, 'r', encoding='utf-8', errors='ignore') as f:\n            self.stopwords.extend([x.rstrip() for x in f.readlines()])\n        self.stopwords = sorted(set(self.stopwords))\n\n        # Vocabulary Dictionary (**custom**)\n        # - for marking keywords to retain during pre-processing step\n        # - modify spacy's stopword checker\n        self.vocab_list += [\"CMI\", \"Child Mind Institute\"]\n        self.vocab_list = list(map(str.lower, self.vocab_list))\n        self.vocab_list = sorted(set(self.vocab_list))\n        # modify stop_words list\n        self.stopwords = set(self.stopwords).difference(self.vocab_list)\n        # custom regex using these vocab\n        self.vocab_dict = {w: \"_\".join(w.split()) for w in self.vocab_list}\n        self.regex_custom = re.compile('|'.join(sorted(map(re.escape, self.vocab_dict), key=len, reverse=True)))\n        # for not_stopword in self.vocab_dict.values():\n        #     nlp.vocab[not_stopword].is_stop = False\n\n        # contractions\n        with open(self.contractions_file, 'r', encoding='utf-8', errors='ignore') as f:\n            self.contractions = dict(json.load(f))\n        # chat-words\n        with open(self.chatwords_file, 'r', encoding='utf-8', errors='ignore') as f:\n            self.chat_words_map_dict, self.chat_words_list = {}, []\n            chat_words = [x.rstrip() for x in f.readlines()]\n            for line in chat_words:\n                cw = line.split(\"=\")[0]\n                cw_expanded = line.split(\"=\")[1]\n                self.chat_words_list.append(cw)\n                self.chat_words_map_dict[cw] = cw_expanded\n            self.chat_words_list = set(self.chat_words_list)\n        # emoticons\n        with open(self.emoticons_file, \"r\") as f:\n            self.emoticons = re.compile(u'(' + u'|'.join(k for k in json.load(f)) + u')')\n        # emojis\n        self.emojis = re.compile(\"[\"\n                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                                   u\"\\U00002702-\\U000027B0\"\n                                   u\"\\U000024C2-\\U0001F251\"\n                                   \"]+\", flags=re.UNICODE)\n        # greetings\n        with open(self.greeting_file, 'r', encoding='utf-8', errors='ignore') as f:\n            self.greeting_words = [x.rstrip() for x in f.readlines()]\n        # signature\n        with open(self.signature_file, 'r', encoding='utf-8', errors='ignore') as f:\n            self.signature_words = [x.rstrip() for x in f.readlines()]\n        # spell-corrector\n        self.spell_checker = SpellChecker()\n        return\n\n    def custom_cleaning(self, text, reset=False):\n        \"\"\" *** Perform custom cleaning here... *** \"\"\"\n        # use global variable \"regex_custom\" in this...\n        key = \"_\"\n        text = str(text).strip().lower()\n        if reset is False:\n            # custom replacement\n            text = re.sub(r\"directed[\\s\\,]*share[\\s\\,]*program(?=[\\s\\(\\[]*dsp[\\s\\)\\]]*)\", \"\", text.lower(), re.IGNORECASE)\n            # compile using a dict of words and their expansions, and sub them if found!\n            match_and_sub = self.regex_custom.sub(lambda x: self.vocab_dict[x.string[x.start():x.end()]], text)\n            return re.sub(r\"([\\s\\n\\t\\r]+)\", \" \", match_and_sub).strip()\n        else:\n            # reverse the change! - use this at the end of preprocessing\n            text = text.replace(key, \" \")\n            return re.sub(r\"([\\s\\n\\t\\r]+)\", \" \", text).strip()\n\n    def clean(self, input_sentences):\n        cleaned_sentences = []\n        for sent in input_sentences:\n\n            # normalize text to \"utf-8\" encoding\n            sent = unicodedata.normalize('NFKD', str(sent)).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n\n            # lowercasing\n            sent = str(sent).strip().lower()\n\n            # <---------- CUSTOM CLEANING -------------->\n            #\n            # Mark imp keywords such as: Domain specific, Question words(wh-words), etc, using \"vocabulary\".\n            # Create a \"vocab_dict\" first.  This dict shall be used to join these keywords (i.e. join them using \"_\" ),\n            # during pre-processing step, and later un-joined.\n            if self.custom:\n                sent = self.custom_cleaning(sent, reset=False)\n            #\n            # <---------- CUSTOM CLEANING -------------->\n\n            # remove Emojis\n            sent = self.emojis.sub(r'', sent)\n\n            # remove emoticons\n            sent = self.emoticons.sub(r'', sent)\n\n            # remove common chat-words\n            sent = \" \".join([self.chat_words_map_dict[w.upper()] if w.upper() in self.chat_words_list else w for w in sent.split()])\n\n            # FIX text\n            sent = ftfy.fix_text(sent)\n\n            # Normalize accented chars\n            sent = unicodedata.normalize('NFKD', sent).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n\n            # Removing <…> web scrape tags\n            sent = re.sub(r\"\\<(.*?)\\>\", \" \", sent)\n\n            # Expanding contractions using contractions_file\n            sent = re.sub(r\"(\\w+\\'\\w+)\", lambda x: self.contractions.get(x.group().lower(), x.group().lower()), sent)\n\n            # Removing web urls\n            sent = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0–9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»\"\"'']))''', \" \", sent)\n\n            # Removing date formats\n            sent = re.sub(r\"(\\d{4}\\-\\d{2}\\-\\d{2}\\s\\d{2}\\:\\d{2}\\:\\d{2}\\s\\:)\", \" \", sent)\n\n            # removing punctuations\n            # - disable them, when sentence structure needs to be retained.\n            sent = re.sub(r\"[\\$|\\#\\@\\*\\%]+\\d+[\\$|\\#\\@\\*\\%]+\", \" \", sent)\n            sent = re.sub(r\"\\'s\", \" \\'s\", sent)\n            sent = re.sub(r\"\\'ve\", \" \\'ve\", sent)\n            sent = re.sub(r\"n\\'t\", \" n\\'t\", sent)\n            sent = re.sub(r\"\\'re\", \" \\'re\", sent)\n            sent = re.sub(r\"\\'d\", \" \\'d\", sent)\n            sent = re.sub(r\"\\'ll\", \" \\'ll\", sent)\n            sent = re.sub(r\"[\\/,\\@,\\#,\\\\,\\{,\\},\\(,\\),\\[,\\],\\$,\\%,\\^,\\&,\\*,\\<,\\>]\", \" \", sent)\n            sent = re.sub(r\"[\\,,\\;,\\:,\\-]\", \" \", sent)      # main puncts\n            # sent = re.sub(r\"[\\!,\\?,\\.]\", \" \", sent)       # sentence delimitters\n\n            # keep only text & numbers\n            # - enable them, when only text and numbers matter\n            # sent = re.sub(r\"\\s+\", \" \", re.sub(r\"[\\\\|\\/|\\||\\{|\\}|\\[|\\]\\(|\\)]+\", \" \", re.sub(r\"[^A-z0-9]\", \" \", str(sent))))\n\n            # correct spelling mistakes\n            # - enable them when english spelling mistakes matter\n            # sent = \" \".join([self.spell_checker.correction(w) if w in self.spell_checker.unknown(sent.split()) else w for w in sent.split()])\n\n            # remove 'modifed' self.stop_words\n            sent = \" \".join(token.text for token in nlp(sent) if token.text not in self.stopwords)\n\n            # lemmatize\n            sent = \" \".join(token.lemma_ for token in nlp(sent) if token.text not in self.stopwords)\n\n            # Removing extra whitespaces\n            sent = re.sub(r\"([\\s\\n\\t\\r]+)\", \" \", sent).strip()\n\n            # <---------- CUSTOM CLEANING -------------->\n            #\n            # revers the custom cleaning step!\n            if self.custom:\n                sent = self.custom_cleaning(sent, reset=True)\n            #\n            # <---------- CUSTOM CLEANING -------------->\n\n            cleaned_sentences.append(sent.strip().lower())\n        return cleaned_sentences","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:11:27.231227Z","iopub.execute_input":"2021-11-26T09:11:27.231468Z","iopub.status.idle":"2021-11-26T09:11:27.787971Z","shell.execute_reply.started":"2021-11-26T09:11:27.231438Z","shell.execute_reply":"2021-11-26T09:11:27.786822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## data pre-processing\npreprocessText = basic_preprocessing(resources_dir_path, vocab_list=vocab_list, custom=True)\n\n# Master data\ndf[clean_text_col] = preprocessText.clean(df[text_col])\n# df[unique_id_col] = [\"ch_{}\".format(i) for i in range(0, len(df))]\ndf[clean_text_col] = df[clean_text_col].fillna(value=\"NONE\")\nprint(\"pre-processing completed.\")","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:11:27.789525Z","iopub.execute_input":"2021-11-26T09:11:27.789774Z","iopub.status.idle":"2021-11-26T09:11:29.024702Z","shell.execute_reply.started":"2021-11-26T09:11:27.789744Z","shell.execute_reply":"2021-11-26T09:11:29.023655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Generic engineering\n\ndf['char_len'] = df[text_col].apply(lambda x: len(str(x)))\ndf['word_len'] = df[text_col].apply(lambda x: len(str(x).split()))\nregunk = re.compile(r\"\\b({})\\b\".format(\"|\".join(greeting_words + signature_words)))\ndf['containsJunkWord'] = df[text_col].apply(lambda x: 1 if re.findall(regunk, x.lower().strip()) else 0)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:11:29.02635Z","iopub.execute_input":"2021-11-26T09:11:29.026731Z","iopub.status.idle":"2021-11-26T09:11:29.040692Z","shell.execute_reply.started":"2021-11-26T09:11:29.026661Z","shell.execute_reply":"2021-11-26T09:11:29.039677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Vectorization","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nimport transformers\nfrom transformers import (\n    AutoConfig,\n    AutoTokenizer,\n    AutoModel,\n    TFAutoModel\n)\n\nWORKING_DIR = Path(\"/kaggle/working\")\nprint('Transformers version',transformers.__version__) # Current version: 2.3.0","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:11:29.042111Z","iopub.execute_input":"2021-11-26T09:11:29.042375Z","iopub.status.idle":"2021-11-26T09:11:29.055879Z","shell.execute_reply.started":"2021-11-26T09:11:29.042347Z","shell.execute_reply":"2021-11-26T09:11:29.054904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Vectorization using Fine-tuned Sentence BERT (3dr corpus 11k documents)\n\nclass BertTransformer(BaseEstimator, TransformerMixin):\n\n    def __init__(self, tokenizer, model, max_length=128, embedding_func: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,):\n        self.tokenizer = tokenizer\n        self.model = model\n        self.model.eval()\n        self.max_length = max_length\n        self.embedding_func = embedding_func\n        if self.embedding_func is None:\n            self.embedding_func = lambda x: x[0][:, 0, :].squeeze()\n\n    def _tokenize(self, text):\n        # Mean Pooling - Take attention mask into account for correct averaging\n        def mean_pooling(model_output, attention_mask):\n            token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n            sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n            return sum_embeddings / sum_mask\n\n        # Tokenize the text with the provided tokenizer\n        encoded_input = tokenizer(text, padding=True, truncation=True, max_length=self.max_length, return_tensors='pt')\n\n        # Compute token embeddings\n        with torch.no_grad():\n            model_output = self.model(**encoded_input)\n\n        # Perform mean pooling\n        sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\n        # bert takes in a batch so we need to unsqueeze the rows\n        return sentence_embeddings\n\n    def transform(self, text: List[str]):\n        if isinstance(text, pd.Series):\n            text = text.tolist()\n        return self._tokenize(text)\n\n    def fit(self, X, y=None):\n        \"\"\"No fitting required so we just return ourselves. For fine-tuning, refer to shared gpu-code!\"\"\"\n        return self","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:11:29.057997Z","iopub.execute_input":"2021-11-26T09:11:29.058707Z","iopub.status.idle":"2021-11-26T09:11:29.073579Z","shell.execute_reply.started":"2021-11-26T09:11:29.05862Z","shell.execute_reply":"2021-11-26T09:11:29.072541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# BERT VECTORIZATION\nm_fp = 'sentence-transformers/all-distilroberta-v1'\n\n# load tokenizer, model classes\ntokenizer = AutoTokenizer.from_pretrained(m_fp)\nmodel_bert = AutoModel.from_pretrained(m_fp)\n\n# load vectorizer\nbert_vectorizer = BertTransformer(tokenizer, model_bert, embedding_func=lambda x: x[0][:, 0, :].squeeze())\nprint(\"Vectorization class loaded.\")","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:11:29.077577Z","iopub.execute_input":"2021-11-26T09:11:29.078101Z","iopub.status.idle":"2021-11-26T09:11:31.756171Z","shell.execute_reply.started":"2021-11-26T09:11:29.078064Z","shell.execute_reply":"2021-11-26T09:11:31.755216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_vectorizer","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:11:31.757359Z","iopub.execute_input":"2021-11-26T09:11:31.7577Z","iopub.status.idle":"2021-11-26T09:11:31.772844Z","shell.execute_reply.started":"2021-11-26T09:11:31.757642Z","shell.execute_reply":"2021-11-26T09:11:31.771717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Contextual simialrity","metadata":{}},{"cell_type":"code","source":"# Textual + Semantic Similarity comparison analysis\nclass Generate_Similarity_Matrix(object):\n\n    def __init__(self, master, duplicates=None, master_id=None, duplicates_id=None, min_similarity=0.80, vectorizer='tfidf'):\n        # UTILITY FUNCTIONS\n        def _is_series_of_strings(series_to_test: pd.Series):\n            if not isinstance(series_to_test, pd.Series): return False\n            elif series_to_test.to_frame().applymap(lambda x: not isinstance(x, str)).squeeze(axis=1).any(): return False\n            return True\n        def _is_input_data_combination_valid(duplicates, master_id, duplicates_id):\n            if duplicates is None and (duplicates_id is not None) or duplicates is not None and ((master_id is None) ^ (duplicates_id is None)): return False\n            else: return True\n\n        # VALIDATE INPUT ARGS\n        if not _is_series_of_strings(master) or (duplicates is not None and not _is_series_of_strings(duplicates)):\n            raise TypeError('Input does not consist of pandas.Series containing only Strings')\n        if not _is_input_data_combination_valid(duplicates, master_id, duplicates_id):\n            raise Exception('List of data Series options is invalid')\n        if master_id is not None and len(master) != len(master_id):\n            raise Exception('Both master and master_id must be pandas.Series of the same length.')\n        if duplicates is not None and duplicates_id is not None and len(duplicates) != len(duplicates_id):\n            raise Exception('Both duplicates and duplicates_id must be pandas.Series of the same length.')\n\n        # SAVE INPUT ARGS\n        self._master = master\n        self._duplicates = duplicates if duplicates is not None else None\n        self._master_id = master_id if master_id is not None else None\n        self._duplicates_id = duplicates_id if duplicates_id is not None else None\n        self.min_similarity = min_similarity\n        self.vectorizer_name = vectorizer     # tfidf, bert\n\n        # CONFIG\n        self._true_max_n_matches = None\n        self._max_n_matches = len(self._master) if self._duplicates is None else len(self._duplicates)\n        self.ngram_size = 3\n        self.regex = r'[,-./]|\\s'\n        self.number_of_processes = multiprocessing.cpu_count() - 1\n        self.DEFAULT_COLUMN_NAME = 'side'\n        self.DEFAULT_ID_NAME = 'id'\n        self.LEFT_PREFIX = 'left_'\n        self.RIGHT_PREFIX = 'right_'\n        self._matches_list = pd.DataFrame()\n        self.is_build = False  # indicates if fit has been called or not\n\n        # -- INIT VECTORIZER --\n        if self.vectorizer_name==\"tfidf\":\n            def get_n_grams(string):\n                if string is not None: string = string.lower()    # lowercasing all str\n                string = re.sub(self.regex, r'', string)\n                n_grams = zip(*[string[i:] for i in range(self.ngram_size)])\n                return [''.join(n_gram) for n_gram in n_grams]\n            # - enable fit() in \"_get_tf_idf_matrices(self)\"\n            self._vectorizer = TfidfVectorizer(min_df=1, analyzer=get_n_grams, dtype=np.float64)\n        if self.vectorizer_name==\"bert\":\n            self._vectorizer = BertTransformer(tokenizer, model_bert, embedding_func=lambda x: x[0][:, 0, :].squeeze())\n        # -- INIT VECTORIZER --\n        return\n\n    def fit(self):\n        \"\"\"\n        Fit a vectorizer (already init) with Master & Duplicates matrix and calculate cosine-sim without original-ids.\n        Params  : Master, Duplicates\n        Return  : dataframe{ Master_Text, Duplicates_Text, cosine_sim(vectorizer_master, vectorized_duplicates) }\n\n        \"\"\"\n        # UTILITY FUNCTIONS\n        def fix_diagonal(m: lil_matrix):\n            r = np.arange(m.shape[0])\n            m[r, r] = 1\n            return m\n        def symmetrize_matrix(m_symmetric: lil_matrix):\n            r, c = m_symmetric.nonzero()\n            m_symmetric[c, r] = m_symmetric[r, c]\n            return m_symmetric\n\n        # Vectorize the matrices\n        # - if duplicate matrix is present use it, else utilize master itself\n        master_matrix, duplicate_matrix = self.get_vectorized_matrices()\n\n        # Calculate cosine similarity b/w master & duplicates (if passed, else use master itself)\n        matches = self.build_matches(master_matrix, duplicate_matrix)\n        self._true_max_n_matches = self._max_n_matches-1\n\n        # Correct sparse matrix multiplcation\n        if self._duplicates is None:\n            # convert to lil format for best efficiency when setting matrix-elements\n            # matrix diagonal elements must be exactly 1 (numerical precision errors introduced by floating-point computations\n            #                                             in awesome_cossim_topn sometimes lead to unexpected results)\n            matches = matches.tolil()\n            matches = fix_diagonal(matches)\n            if self._max_n_matches < self._true_max_n_matches:\n                matches = symmetrize_matrix(matches)\n            matches = matches.tocsr()\n\n        # Create the basic \"matches\" dataframe with \"Master, Duplicate and Similarity\" cols only\n        r, c = matches.nonzero()\n        self._matches_list = pd.DataFrame({'master_side': r.astype(np.int64), 'dupe_side': c.astype(np.int64), 'similarity': matches.data})\n        self.is_build = True\n        return self\n\n    def get_vectorized_matrices(self):\n        \"\"\"\n        Vectorize matrices using one of the vectorizers.\n        Params    : Master, Duplicates, Vectorizer_name(\"tfidf\", \"bert\")\n        Return    : vectorizer_master, vectorized_duplicates\n        \"\"\"\n        def fit_vectorizer():\n            # if both master & duplicates series are set - concat them to fit the vectorizer on all strings at once!\n            if self._duplicates is not None:\n                strings = pd.concat([self._master, self._duplicates])\n            else:\n                strings = self._master\n            self._vectorizer.fit(strings)\n            return self._vectorizer\n\n        if self.vectorizer_name==\"tfidf\":\n            print(\"tfidf\")\n            self._vectorizer = fit_vectorizer()\n            master_matrix = self._vectorizer.transform(self._master)\n            if self._duplicates is not None:\n                duplicate_matrix = self._vectorizer.transform(self._duplicates)\n            else:\n                # IF there is no duplicate matrix, match on the master matrix itself!\n                duplicate_matrix = master_matrix\n\n        if self.vectorizer_name==\"bert\":\n            print(\"bert\")\n            master_matrix = self._vectorizer.transform(self._master)\n            # --> Convert Tensor Matrices to CSR (np.float64)\n            master_matrix = csr_matrix( F.normalize(master_matrix).numpy().astype(np.float64) )\n            if self._duplicates is not None:\n                duplicate_matrix = self._vectorizer.transform(self._duplicates)\n                duplicate_matrix = csr_matrix( F.normalize(duplicate_matrix).numpy().astype(np.float64) )\n            else:\n                # IF there is no duplicate matrix, match on the master matrix itself!\n                duplicate_matrix = master_matrix\n\n        return master_matrix, duplicate_matrix\n\n    def build_matches(self, master_matrix, duplicate_matrix):\n        \"\"\"\n        Builds the cosine similarity matrix of two CSR matrices.\n        Params   : vectorizer_master, vectorized_duplicates\n        Return   : cosine_sim(vectorized_master, vectorized_duplicates)\n        \"\"\"\n        # Matrix A, B\n        tf_idf_matrix_1 = master_matrix\n        tf_idf_matrix_2 = duplicate_matrix.transpose()\n\n        # Calculate cosine similarity\n        optional_kwargs = {'use_threads': self.number_of_processes > 1, 'n_jobs': self.number_of_processes}\n        cosine_sim_matrix = awesome_cossim_topn(tf_idf_matrix_1,\n                                                tf_idf_matrix_2,\n                                                self._max_n_matches,\n                                                self.min_similarity,\n                                                **optional_kwargs)\n        return cosine_sim_matrix\n\n    def get_matches(self):\n        \"\"\"\n        Creates the complete dataframe with index matching(ids) if passed.\n        Params  : dataframe\n        Return  : dataframe{ Master_ids, Master_Text, cosine_similarity, Duplicate_ids, Duplicates_Text }\n        \"\"\"\n        # UTILITY FUNCTIONS\n        def get_both_sides(master, duplicates, generic_name=(self.DEFAULT_COLUMN_NAME, self.DEFAULT_COLUMN_NAME), drop_index=False):\n            lname, rname = generic_name\n            left = master if master.name else master.rename(lname)\n            left = left.iloc[matches_list.master_side].reset_index(drop=drop_index)\n            if self._duplicates is None:\n                right = master if master.name else master.rename(rname)\n            else:\n                right = duplicates if duplicates.name else duplicates.rename(rname)\n            right = right.iloc[matches_list.dupe_side].reset_index(drop=drop_index)\n            return left, (right if isinstance(right, pd.Series) else right[right.columns[::-1]])\n        def prefix_column_names(data, prefix):\n            if isinstance(data, pd.DataFrame):\n                return data.rename(columns={c: f\"{prefix}{c}\" for c in data.columns})\n            else:\n                return data.rename(f\"{prefix}{data.name}\")\n\n        if self.min_similarity > 0:\n            matches_list = self._matches_list\n        else:\n            raise Exception(\"min_similarity cannot be set to less than or equal to 0!\")\n\n        # ID Retrival\n        left_side, right_side = get_both_sides(self._master, self._duplicates, drop_index=False)\n        similarity = matches_list.similarity.reset_index(drop=True)\n        if self._master_id is None:\n            # if ids are not passed\n            return pd.concat([prefix_column_names(left_side, self.LEFT_PREFIX),\n                              similarity,\n                              prefix_column_names(right_side, self.RIGHT_PREFIX)], axis=1)\n\n        else:\n            # if ids are passed, retrive ids\n            left_side_id, right_side_id = get_both_sides(self._master_id, self._duplicates_id, (self.DEFAULT_ID_NAME, self.DEFAULT_ID_NAME), drop_index=True)\n            return pd.concat([prefix_column_names(left_side, self.LEFT_PREFIX),\n                              prefix_column_names(left_side_id, self.LEFT_PREFIX),\n                              similarity,\n                              prefix_column_names(right_side_id, self.RIGHT_PREFIX),\n                              prefix_column_names(right_side, self.RIGHT_PREFIX)], axis=1)\n\ndef match_strings(master, duplicates=None, master_id=None, duplicates_id=None, min_similarity=0.80, vectorizer=None):\n    \"\"\" Find pair-wise similarity b/w Master & Duplicate Matrices. \"\"\"\n    if vectorizer and vectorizer.lower().strip() in ['tfidf', 'bert']:\n        vectorizer=vectorizer.lower().strip()\n        cos_sim_matrix = Generate_Similarity_Matrix(master, duplicates=duplicates, master_id=master_id, duplicates_id=duplicates_id, min_similarity=min_similarity, vectorizer=vectorizer)\n        cos_sim_matrix.fit()                     # run vectorizer & generate basic pair-wise cosine sim matrix\n        sim_df = cos_sim_matrix.get_matches()    # add ids if passed to sim matrix\n        return sim_df\n    else:\n        raise Exception(\"Vectorizer is not passed or incorrect! Please select one: [tfidf, bert]\")","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:11:31.774759Z","iopub.execute_input":"2021-11-26T09:11:31.775622Z","iopub.status.idle":"2021-11-26T09:11:31.839344Z","shell.execute_reply.started":"2021-11-26T09:11:31.775576Z","shell.execute_reply":"2021-11-26T09:11:31.838341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Run Similarity Computation on 1 master file","metadata":{}},{"cell_type":"code","source":"def run_Similarity_on_MasterFile(df, master_min_similarity=0.75):\n\n    # 1. collect duplicate ids based on \"text\" col\n    def collect_dups(data, id_col, dup_col, output_col_name):\n        dup_dict = data.reset_index().groupby(data[dup_col].tolist())[id_col].agg(list).reset_index().reset_index(drop=True).rename(columns={\"index\": dup_col, id_col: output_col_name})\n        dup_dict = dup_dict.set_index(dup_col)[output_col_name].to_dict()\n        data[output_col_name] = data[dup_col].apply(lambda txt: dup_dict[txt])\n        return data\n\n    # 2. drop dup ids, keep first\n    def drop_dups(data, col):\n        return data.drop_duplicates(subset=[col]).reset_index(drop=True)\n\n    # 3. collect similar pairs\n    def pairwise_similarity_matrix(data, id_col, text_col, similar_id_col, min_similarity=0.75):\n        # TEXTUAL SIMILARITY\n\n        # MODULE 1 :: pair-wise textual similarity\n        matches = match_strings(master=data[text_col], master_id=data[id_col], min_similarity=min_similarity, vectorizer='bert')\n\n        # group similar-pairs together (left-join)\n        left_col_name, left_unique_id, right_unique_id = \"left_%s\" % text_col, \"left_%s\" % id_col, \"right_%s\" % id_col\n        \n        match_df = matches.groupby([left_col_name, left_unique_id])[right_unique_id].agg(similar_idx=lambda x: sorted(set(x))).reset_index().sort_values(by=[left_unique_id], ascending=True).reset_index(drop=True)\n\n        # asthestic: drop dummy added left/right names\n        matches = matches.drop(columns=['left_index', \"right_index\"])\n        match_df = match_df.rename(columns={left_unique_id: id_col, left_col_name: text_col})\n        \n        print(match_df.columns)\n        print(match_df.head(3))\n        \n        return matches, match_df\n\n    ## Utility: alphanumeric sort\n    _nsre = re.compile('([0-9]+)')\n    def natural_sort_key(s):\n        return [int(text) if text.isdigit() else text.lower()\n                for text in re.split(_nsre, s)]\n\n    # 4. create \"dup_similar_idx\" col - merge dup_id data with similar_id data\n    def combine_dup_similar(data, match_df, id_col, dup_id_col, similar_id_col, dup_sim_id_col):\n\n        # merge df_duplicates with df_similar\n        cols_to_use = [id_col, similar_id_col]\n        data = data.merge(match_df[cols_to_use], on=id_col, how='outer')\n\n        # create combined list: dup_ids + similar_ids\n        # --> \"dup_similar_id_col\" == \"duplicated_pairs_idx\" + \"similar_pairs_idx\n        data[dup_sim_id_col] = [sorted(set(sum(tup, []))) for tup in zip(data[dup_id_col], data[similar_id_col])]\n\n        # custom sorting (to handle alphanumeric ids)\n        if isinstance(data[dup_sim_id_col][0], str):\n            data[dup_sim_id_col] = data[dup_sim_id_col].apply(lambda x: sorted(x, key=natural_sort_key))\n        return data\n\n    # 5. merged all nested lists containing common sub-elements in \"dup_similar_id\" cols\n    def collect_similar_ids(data, id_col, dup_id_col, similar_id_col, dup_sim_id_col):\n\n        # collect nested list which needs to be merged\n        list_similar_ids = list(map(list, data[dup_sim_id_col]))\n\n        # merge all nested lists with common elements\n        g = nx.Graph()\n        edges = [g.add_edges_from(zip(p, p[1:])) if len(p)>1 else g.add_edges_from(zip(p, p[:])) for p in list_similar_ids]\n        merged_similar_idx = [sorted(c) for c in nx.connected_components(g)]\n\n        # create two mappings, one for storing cluster_id: list of ids, and one inverted dict\n        # --> \"id_clus_dict\" is the cluster id mapping for each 'unique_id'\n        temp_id = 1\n        clus_id_dict = {}  # cluster_1: merged([id1, id2,..., idn])\n        id_clus_dict = {}  # merged(id1): cluster_1; merged(id1): cluster_1; .., merged(idn): cluster_1\n        for lst in merged_similar_idx:\n            key = \"cluster_%s\"%temp_id\n            for value in lst:\n                id_clus_dict[value] = key\n            clus_id_dict[key]=lst\n            temp_id+=1\n\n        # assign dup_similar_idx based on two mappings above\n        df[dup_sim_id_col] = df[id_col].apply(lambda uid: clus_id_dict[id_clus_dict[uid]])\n\n        # create duplicate id mapping and similar id mapping files\n        dup_id_dict = {_id: ids for ids in df[dup_id_col].tolist() for _id in ids}\n        sim_id_dict = {_id: ids for ids in df[similar_id_col].tolist() for _id in ids}\n        dup_sim_id_dict = {_id: ids for ids in df[dup_sim_id_col].tolist() for _id in ids}\n\n        # custom sorting (to handle alphanumeric ids)\n        if isinstance(data[dup_sim_id_col][0], str):\n            df[dup_sim_id_col] = df[dup_sim_id_col].apply(lambda x: sorted(x, key=natural_sort_key))\n        return data, clus_id_dict, id_clus_dict, dup_id_dict, sim_id_dict, dup_sim_id_dict\n\n    # 6. Drop duplicates based on dup_similar_id_col, i.e. duplicated_id + similar_ids\n    def create_final_single_matrix(data, dup_sim_id_col):\n        data[dup_sim_id_col] = tuple(map(tuple, data[dup_sim_id_col]))\n        data = data.drop_duplicates(subset=[dup_sim_id_col]).reset_index(drop=True)\n        data[dup_sim_id_col] = list(map(list, data[dup_sim_id_col]))\n        return data\n\n    # 7. Expand each id to assign clusters\n    def create_clusters(data, id_col, idx_cluster_map):\n        data['cluster_id'] = data[id_col].apply(lambda uid: idx_cluster_map.get(uid, -1))\n        return data\n\n    # 8. Display Analytics\n    def run_stats():\n        print(\"Stats:\\n\"\n          \"\\nOrignal number of records = {}\"\n          \"\\nTotal dup count = {}\"\n          \"\\nTotal similar pairs found = {}\"\n          \"\\nFinal number of records post dup-similar rows removal = {}\".format(len(original_df),\n                                                                              len(sum(df[master_dup_id_col].tolist(), [])),\n                                                                              len(sum(df[master_similar_id_col].tolist(), [])),\n                                                                              len(df)))\n    \"\"\"\n    Run similarity computation on pre-processed Master file involving Dup identification, collection & removal, finally running\n    similarity analysis on remaining unique rows.\n    param    : dataframe (single file containing - 'unique_id', 'cleaned_text')\n    return   : final dataframe with only unique rows, original dataframe with results (cluster info)\n    \"\"\"\n    ## EXECUTE ##\n    original_df = df.copy()\n    df = collect_dups(df, master_id_col, master_clean_text_col, master_dup_id_col)\n    df = drop_dups(df, master_clean_text_col)\n    matches, match_df = pairwise_similarity_matrix(df, master_id_col, master_clean_text_col, master_similar_id_col, min_similarity=master_min_similarity)\n    df = combine_dup_similar(df, match_df, master_id_col, master_dup_id_col, master_similar_id_col, master_dup_similar_id_col)\n    df, cluster_id_map, idx_cluster_map, dup_id_dict, sim_id_dict, dup_sim_id_dict = collect_similar_ids(df,  master_id_col, master_dup_id_col, master_similar_id_col, master_dup_similar_id_col)\n    df = create_final_single_matrix(df, master_dup_similar_id_col)\n    df['cluster_id'] = df[master_id_col].apply(lambda uid: idx_cluster_map.get(uid, -1))\n    # save back in original df (without dups or similar dropped!)\n    original_df['dup_idx'] = original_df[master_id_col].apply(lambda uid: dup_id_dict.get(uid, -1))\n    original_df['similar_idx'] = original_df[master_id_col].apply(lambda uid: sim_id_dict.get(uid, -1))\n    original_df['dup_similar_idx'] = original_df[master_id_col].apply(lambda uid: dup_sim_id_dict.get(uid, -1))\n    original_df['cluster_id'] = original_df[master_id_col].apply(lambda uid: idx_cluster_map.get(uid, -1))\n    run_stats()\n    return original_df","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:11:31.840888Z","iopub.execute_input":"2021-11-26T09:11:31.841161Z","iopub.status.idle":"2021-11-26T09:11:31.880499Z","shell.execute_reply.started":"2021-11-26T09:11:31.84113Z","shell.execute_reply":"2021-11-26T09:11:31.879611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# :: config ::\n\nmaster_id_col = 'ID'\nmaster_text_col = 'Corpus'\nmaster_clean_text_col = 'clean_text'\nmaster_dup_id_col = \"dup_idx\"\nmaster_similar_id_col = \"similar_idx\"\nmaster_dup_similar_id_col = \"dup_similar_idx\"\n\n# drop na\ndf[master_clean_text_col] = df[master_clean_text_col].fillna(value=\"NONE\")\nprint(\"Settings set into config.\")","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:11:31.881971Z","iopub.execute_input":"2021-11-26T09:11:31.882948Z","iopub.status.idle":"2021-11-26T09:11:31.902592Z","shell.execute_reply.started":"2021-11-26T09:11:31.882903Z","shell.execute_reply":"2021-11-26T09:11:31.901643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get conxtetually similar records\n\ndf = run_Similarity_on_MasterFile(df, master_min_similarity=0.75)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:11:31.904306Z","iopub.execute_input":"2021-11-26T09:11:31.904627Z","iopub.status.idle":"2021-11-26T09:11:34.729243Z","shell.execute_reply.started":"2021-11-26T09:11:31.904583Z","shell.execute_reply":"2021-11-26T09:11:34.728378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Iterative Clustering","metadata":{}},{"cell_type":"code","source":"### Clustering Algorithm\n\nclass doc_clustering():\n\n    def __init__(self, resource_dir, spacy_model, vectorizer):\n        self.stopwd_file = os.path.join(resource_dir, \"stopwords.txt\")\n        self.spacy_model = spacy_model\n        self.vectorizer = vectorizer.strip().lower()\n        # :: clustering config ::\n        self.clustering_type = \"noun\"\n        self.kmeans_rate = KMEANS_RATE\n        self.cluster_k = CLUSTER_K\n        self.kmeans_seed_init = KMEANS_SEED_INIT\n        self.kmeans_maxiter = KMEANS_MAXITER\n        self.cluster_length = CLUSTER_LEN\n        self.cohesion_threshold = COHESION_THRESHOLD\n        # :: resource config ::\n        self.stopwords = []\n        self.load_resources()\n        self.test_mode = \"no\"\n\n    def load_resources(self):\n        # load stopwords\n        with io.open(self.stopwd_file, 'r', encoding='utf-8', errors='ignore') as f:\n            self.stopwords = [x.rstrip() for x in f.readlines()]\n        return\n\n    def get_pos_list(self, results):\n        # POS Annotation using Spacy\n        pos_ans = []\n        word_list = []\n        lemma_list = []\n        ne_list = []\n        start_end_list = []\n        indices = results['sentences']\n        for line in indices:\n            tokens = line['tokens']\n            for token in tokens:\n                pos_syn = \"\"\n                pos = token['pos'].lower()\n                pos_ans.append(token['pos'])\n                word_list.append(token['word'])\n                lemma = token['lemma'].lower()\n                if lemma in self.stopwords and lemma not in ['want', 'against', 'further', 'online', 'same', 'under',\n                                                             'what', 'want', 'when', 'own', ''] \\\n                        or lemma in [\":\", \"-lrb-\", \"-rrb-\", \"-lsb-\", \"-rsb-\", \"\\\\\", '-pron-', '_', 'card num', '\"'] \\\n                        or pos == \":\" or pos == \".\" or re.search('^([\\W]*)$]', lemma) or len(lemma) >= 30:\n                    continue\n                if re.search('^nn', pos):\n                    pos_syn = 'noun'\n                elif re.search('^v', pos):\n                    pos_syn = 'verb'\n                elif re.search('^adj', pos):\n                    pos_syn = 'adj'\n                lemma_list.append(lemma)\n                ne_list.append(token['ner'])\n                start_end_list.append(str(token['characterOffsetBegin']) + \"_\" + str(token['characterOffsetEnd']))\n\n        if len(lemma_list) == 0:\n            sent_lemma = \"NO_NP_KEYS\"\n        else:\n            sent_lemma = \" \".join(lemma_list)\n        return \" \".join(word_list), sent_lemma, \" \", \" \"\n\n    def get_sent_lemma(self, sent):\n        # allowed spacy attributes\n        props_str = [{'value': 'false', 'key': 'enforceRequirements'},\n                     {'value': 'json', 'key': 'outputFormat'},\n                     {'value': 'normalizeSpace=false, strictTreebank3=true', 'key': 'tokenize.options'},\n                     {'value': 'tokenize,ssplit,pos,lemma,ner', 'key': 'annotators'},\n                     {'value': 'true', 'key': 'ssplit.eolonly'}]\n        input_json = {'text': sent, 'props': props_str}\n        results = run_pipeline(input_json, self.spacy_model)\n        return self.get_pos_list(results)\n\n    def run_doc_clustering(self, documents, k_val, run_analysis=False):\n\n        max_iter = self.kmeans_maxiter\n        ninit = self.kmeans_seed_init\n        r_num = len(documents) - 1\n\n        # Vectorization\n        if self.vectorizer==\"tfidf\":\n            vectorizer = TfidfVectorizer(use_idf=True, ngram_range=(1, 1))\n            X = vectorizer.fit_transform(documents)\n            print(\"Vectorizing using...tfidf\")\n        elif self.vectorizer==\"bert\":\n            X = F.normalize(bert_vectorizer.transform(documents), p=2, dim=1)\n            X = csr_matrix(X.numpy().astype(np.float64))\n            print(\"Vectorizing using...bert\")\n        else:\n            raise Exception(\"ERROR: Please specify a vectorizer to use: options: ['tfidf', 'bert']\")\n        print(\"Vectorization complete!\")\n\n        # Value of \"K\" clusters\n        true_k = k_val = int(k_val)\n\n        ## --> Heusristics <--\n        if self.clustering_type == \"fixed\": true_k = 2000\n        elif self.clustering_type == \"noun\": true_k = int(np.sqrt(len(documents) / 2)) * self.kmeans_rate\n        elif self.clustering_type == \"verb\": true_k = int(np.sqrt(len(documents) / 2))\n        else: print(\"Please set up clustering type! Options=['fixed', 'noun', 'verb']\")\n        print(\"Heusristic value of K: true_k=\", true_k)\n\n        ## --> Elbow method (silhouette) <--\n        if run_analysis and k_val==0 and true_k>=25:\n            search_range = range(true_k-20, true_k+20, 1)\n            sil_score_max = -1 # minimum possible score\n            for n_clusters in search_range:\n                model = KMeans(n_clusters=n_clusters, init='k-means++', max_iter=2000, random_state=r_num, n_init=ninit)\n                labels = model.fit_predict(X)\n                sil_score = silhouette_score(X, labels)\n                print(\"The average silhouette score for %i clusters is %0.2f\" %(n_clusters, sil_score))\n                if sil_score > sil_score_max:\n                    true_k, sil_score_max = n_clusters, sil_score\n            print(\"Silhouette Analysis completed! Best_n_clusters K=\", true_k)\n\n        if k_val > 0 and self.clustering_type == \"noun\":\n            if len(documents) / 2 < k_val:\n                debug_msg = \"Number of input lines: {}\\n\" +                              \"K value provided: {}\\n\" +                              \"Suggested value of K for the 1st level clustering is less than input-lines divided by 2\\n\" +                              \"Please try again!  Heusristic value of K (if line number > 100): {}\"                            .format(len(documents), k_val, int(np.sqrt(len(documents)/2))*self.kmeans_rate)\n                raise Exception(debug_msg)\n            else:\n                true_k = k_val\n\n        # Final Clustering\n        if true_k == 0: true_k = 1\n        model = KMeans(n_clusters=true_k, init='k-means++', max_iter=2000, random_state=r_num, n_init=ninit)\n        model.fit(X)\n\n        cluster_labels = model.labels_\n        cluster_centers = model.cluster_centers_\n        print(\"Clustering is complete.\")\n\n        return model, cluster_labels, cluster_centers, X","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:11:34.730992Z","iopub.execute_input":"2021-11-26T09:11:34.731511Z","iopub.status.idle":"2021-11-26T09:11:34.766627Z","shell.execute_reply.started":"2021-11-26T09:11:34.731465Z","shell.execute_reply":"2021-11-26T09:11:34.765714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Clustering Utility","metadata":{}},{"cell_type":"code","source":"def cluster_cohesion_cosine(clustering, cluster_set_by_sid, data):\n    '''\n    :: COESHION SCORE ::\n    Calculates cohesion score for each cluster id. Accepts 1 cluster with all members, and re-runs clustering on them,\n    with k=1, e.g. KMeans(cluster_id_1_documents, k=1). Calculates relative distances between each member.\n    '''\n    document_center = []\n    values = []\n    for sid in cluster_set_by_sid:\n        document_center.append(data[sid]['sent_lemma'])\n\n    # clustering one cluster members with k=1 to find relatvie distances\n    model_center, cluster_labels_, cluster_centers_center, X_center = clustering.run_doc_clustering(document_center, 1)\n\n    for i in range(0, len(cluster_set_by_sid)):\n        d_vec = X_center[i].toarray()\n        simval = spatial.distance.cosine(d_vec, cluster_centers_center[0,:])\n        if math.isnan(simval):\n            values.append(0)\n        else:\n            values.append(1 - simval)\n    if len(values) == 0:\n        cohesion_score = 0\n    else:\n        # avg distance of all members\n        cohesion_score = sum(values) / len(values)\n    return cohesion_score\n\ndef cluster_center_sent(clustering, cluster_set_by_sid, data):\n    '''\n    :: CLUSTER CENTRE ::\n    Calculates cluster centre using model capability to return cluster centre values, and calculates closest two members.\n    '''\n    document_center = []\n    sid_mapping_center = {}\n    sid_tmp = 0\n    sent_num_cid = len(cluster_set_by_sid)\n\n    dup_sid_set = []\n    for sid in cluster_set_by_sid:\n        sid_mapping_center[sid_tmp] = sid\n        if len(data[sid]['sent_lemma']) > 0:\n            dup_sid_set.append(sid)\n        document_center.append(data[sid]['sent_lemma'])\n        sid_tmp +=1\n\n    if len(dup_sid_set) == 1:\n        seedid1 = list(dup_sid_set)[0]\n        return seedid1, seedid1\n\n    # clustering one cluster members with k=1 to find relatvie distances\n    model_center, cluster_labels_, cluster_centers_center, X_center = clustering.run_doc_clustering(document_center, 1)\n\n    DISTMIN = -9999\n\n    values = []\n    values_sid_set = []\n    # remove duplicated ones\n    for i in range(0, sent_num_cid):\n        sid_data = sid_mapping_center[i]\n        if data[sid_data][\"count\"] > 0:\n            d_vec = X_center[i].toarray()\n            # values.append(spatial.distance.euclidean(d_vec, cluster_centers_center[0,:]))\n            if math.isnan(spatial.distance.cosine(d_vec, cluster_centers_center[0,:])):\n                values.append(1)\n            else:\n                values.append(1 - spatial.distance.cosine(d_vec, cluster_centers_center[0,:]))\n            # score = 1 - spatial.distance.cosine(d_vec, cluster_centers[int(cid),:]) #mainid\n            values_sid_set.append(sid_data)\n\n    seedid1_tmp = values.index(max(values))\n    min_dist1 = values[seedid1_tmp]\n    values[seedid1_tmp] = DISTMIN\n    seedid1 = values_sid_set[ seedid1_tmp ]\n\n    seedid2_tmp = values.index(max(values))\n    min_dist2 = values[seedid2_tmp]\n    seedid2 = values_sid_set[ seedid2_tmp ]\n    values[seedid1_tmp] = min_dist1\n\n    return seedid1, seedid2, values, values_sid_set","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:11:34.768044Z","iopub.execute_input":"2021-11-26T09:11:34.768309Z","iopub.status.idle":"2021-11-26T09:11:34.789832Z","shell.execute_reply.started":"2021-11-26T09:11:34.768278Z","shell.execute_reply":"2021-11-26T09:11:34.788924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Clustering Pipeline","metadata":{}},{"cell_type":"code","source":"def run_clustering(df_cluster, spacy_model, preprocess=False, vectorizer='bert'):\n\n    # init clustering object (alogrithm)\n    clustering = doc_clustering(resources_dir_path, spacy_model, vectorizer)\n\n    start = time.time()\n    ################################################################################################\n    # 1. PREPARE DATA\n    ################################################################################################\n\n    print(\"Uploading input file and lemmatizing...\")\n    # create 'data' dict to store every info regarding each sentence\n    data = OrderedDict()\n\n    for index, row in df_cluster.iterrows():\n        uid, input_text, dup_similar_idx = row[master_id_col], row[master_clean_text_col], row[master_dup_similar_id_col]\n        if len(input_text.strip())==0: input_text = 'NONE'\n        original_text = input_text.strip()\n\n        if preprocess:\n            # run spacy complete pipeline\n            sent_tokens, sent_lemma, sent_verb, sent_dup = clustering.get_sent_lemma(original_text)\n            input_text = str(sent_lemma)\n\n        # features\n        upper_cnt = sum(1 for c in input_text if c.isupper())\n        case_ratio = float(upper_cnt) / float(len(input_text))\n        data.setdefault(index, {})[\"sent_lemma\"] = str(input_text)\n        # :not: data.setdefault(index, {})[\"sent_noun\"] = str(sent_lemma)\n        # :not: data.setdefault(index, {})[\"sent_verb\"] = str(sent_verb)\n        # :not: data.setdefault(index, {})[\"sent_tokens\"] = sent_tokens\n        data.setdefault(index, {})[\"count\"] = len(dup_similar_idx)\n        data.setdefault(index, {})[\"case_ratio\"] = case_ratio\n        data.setdefault(index, {})[\"clusterid\"] = \"no\"\n        data.setdefault(index, {})[\"keep\"] = \"yes\"\n        data.setdefault(index, {})[\"select\"] = \"no\"\n        data.setdefault(index, {})[\"index\"] = index\n        data.setdefault(index, {})[\"unique_id\"] = uid\n        data.setdefault(index, {})[\"sent_raw\"] = original_text\n\n    ################################################################################################\n    # 2. PREPARE CORPUS\n    ################################################################################################\n    print(\"Collecting text corpus\")\n    documents = []\n    main_docid_id_map = {}  # id in documents vs real id in input\n    doc_id = 0\n    for id, x in list(data.items()):\n        if data[id][\"count\"] >= 0:\n            main_docid_id_map[doc_id] = id\n            documents.append(data[id]['sent_lemma'])\n            doc_id += 1\n\n    if len(documents) < 10:\n        raise Exception(\"The file contains sentences less than 10! Please check  and run again!\")\n    else:\n        print('>> Total input sentences = ', len(documents))\n\n    ################################################################################################\n    # 3. 1st LEVEL CLUSTERING\n    ################################################################################################\n    print(\"\\nPerforming 1st level clustering\")\n\n    # init, using heursitics to find value of k\n    cluster_k = 0\n\n    clustering.clustering_type = \"noun\"\n    model, cluster_labels, cluster_centers, X = clustering.run_doc_clustering(documents, cluster_k, run_analysis=False)\n\n    # cluster_id : question id\n    c_s_list = [(y,x) for x,y in enumerate(cluster_labels)]\n\n    ################################################################################################\n    # 4. COLLECT CLUSTERED IDs\n    ################################################################################################\n    print(\"\\nCollecting clustered ids\")\n\n    # add first level cluster-id to data dict\n    # --> cluster_id : [Q_id1, Q_id2, ..., Q_idn]\n    cid_sid_counter = {}\n    for cid, sid in c_s_list:\n        main_sid = main_docid_id_map[sid]\n        data.setdefault(main_sid, {})[\"clusterid\"] = str(format(cid, \"05d\"))\n        cid_sid_counter.setdefault(cid, []).append(main_sid)\n\n    ################################################################################################\n    # 5. CHECK REQUIREMENT FOR 2nd LEVEL CLUSTERING\n    ################################################################################################\n    print(\"\\nFine-tuning clusters: Checking if 2nd level clustering is required...\")\n\n    clustering.clustering_type = \"noun\"\n    cohesionThreshold = clustering.cohesion_threshold\n    clusteringMaxLen = clustering.cluster_length\n    print('cohesionThreshold==', cohesionThreshold)\n\n    for cid in sorted(cid_sid_counter.keys()):\n\n        # --> documents under cluster_id = 'cid'\n        documents_sub =[]\n        for sid in cid_sid_counter[cid]:\n            documents_sub.append(data[sid]['sent_lemma'])\n\n        if len(documents_sub) > clusteringMaxLen:\n            cohesion_score = cluster_cohesion_cosine(clustering, cid_sid_counter[cid], data)\n            print('cid -- cohesion_score = ', cid, cohesion_score)\n\n            # RE-CLUSTERING   (to use cohesion value for basis of reclustering)\n            if cohesion_score < cohesionThreshold:\n                print('**re-clustering: cluster_id={}; member count={}; cohesion={}'.format(cid, len(documents_sub), cohesion_score))\n                model_sub, cluster_labels_sub, cluster_centre_sub, X_sub = clustering.run_doc_clustering(documents_sub, cluster_k)\n            else:\n                continue\n        else:\n            continue\n\n        # :: ONLY if 2nd clustering was performed ::\n        c_s_list_sub = [(y,x) for x,y in enumerate(cluster_labels_sub)]\n\n        # add 2nd level cluster-id to data dict\n        # --> cluster_id : [Q_id1, Q_id2, ..., Q_idn]\n        for cid_sub,sid_sub in c_s_list_sub:\n            sid_index = cid_sid_counter[cid][sid_sub]\n            data[sid_index]['clusterid'] = data[sid_index]['clusterid'] + \"_\"+str(format(cid_sub, \"03d\"))\n\n    # reset to first level\n    clustering.clustering_type = \"noun\"\n    cluster_set_by_sid = {}\n    for id, x in list(data.items()):\n        if data[id]['clusterid'] != \"no\":\n            cid = data[id]['clusterid']\n            cluster_set_by_sid.setdefault(cid, []).append(id)\n\n    ################################################################################################\n    # 6. FILTER, FIND INSIGHTS & SAVE\n    ################################################################################################\n    print(\"\\nFiltering, getting insights and saving output\")\n\n    # SAVE OUTPUT (CSV)\n    output=[]\n    output.append(\"{}\\t{}\\tText\\tSeed_Q1\\tSeed_Q2\\tDist_to_Center\\tCohesion\\r\\n\".format(master_clustering_col,master_id_col))\n\n    T = 0.0  # recommended: 0.4\n    cid_index = 0\n    cluster_set_by_sid_filtered = {}\n    cluster_set_by_sid_filtered_coh_score = {}\n    for cid in list(cluster_set_by_sid.keys()):\n        if len(cluster_set_by_sid[cid]) > 1:\n            cohesion_score = cluster_cohesion_cosine(clustering, cluster_set_by_sid[cid], data)\n            if cohesion_score < T:\n                for s in cluster_set_by_sid[cid]:\n                    cluster_set_by_sid_filtered.setdefault(cid_index,[]).append(s)\n                    cid_index += 1\n            else:\n                cluster_set_by_sid_filtered[cid_index] = cluster_set_by_sid[cid]\n                cluster_set_by_sid_filtered_coh_score[cid_index] = cohesion_score\n                cid_index += 1\n        else:\n            cluster_set_by_sid_filtered[cid_index] = cluster_set_by_sid[cid]\n            cluster_set_by_sid_filtered_coh_score[cid_index] = 0\n            cid_index += 1\n\n    output_index=0\n    for cid in list(cluster_set_by_sid_filtered.keys()):\n        sent_num_cid = len(cluster_set_by_sid_filtered[cid])\n        values = []\n        values_sid = []\n\n        if sent_num_cid > 0 and len(cluster_set_by_sid_filtered[cid]) > 1:\n            seedid1,seedid2, values, values_sid = cluster_center_sent(clustering, cluster_set_by_sid_filtered[cid], data)\n        else:\n            seedid1 = cluster_set_by_sid_filtered[cid][0]\n            seedid2 = cluster_set_by_sid_filtered[cid][0]\n            values.append(1)\n            values_sid.append(seedid1)\n\n        seed_q1 = data[seedid1][\"sent_raw\"]\n        seed_q2 = data[seedid2][\"sent_raw\"]\n\n        sent_count = 0\n        for sid in cluster_set_by_sid_filtered[cid]:\n            sent_count += data[sid][\"count\"]\n        ch_score= cluster_set_by_sid_filtered_coh_score[cid]\n\n        for sid in cluster_set_by_sid_filtered[cid]:\n            dup_count = data[sid]['count']\n            if dup_count == 0:\n                continue\n            else:\n                sid_norm = data[sid][\"index\"]\n                sent = data[sid_norm]['sent_raw']\n            sim_index = values_sid.index(sid)\n            dist_to_center = 1.0 - values[sim_index]\n            unique_id = data[sid][\"unique_id\"]\n\n            # cid, sid, unique_id, sent, seed_q1, seed_q2, dist_to_center, cohesion_score\n            output.append(\"%d\\t%s\\t%s\\t%s\\t%s\\t%5.3f\\t%5.3f\\r\\n\" % (cid, unique_id, sent, seed_q1, seed_q2, dist_to_center, ch_score))\n        cid_index += 1\n    clustered_output = pd.read_csv(StringIO(\"\\n\".join(output)), sep=\"\\t\").reset_index(drop=True)\n    print(\"\\nClustering process finished! Time taken(s) =\", (time.time()-start))\n\n    return clustered_output","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:11:34.791779Z","iopub.execute_input":"2021-11-26T09:11:34.792322Z","iopub.status.idle":"2021-11-26T09:11:34.836051Z","shell.execute_reply.started":"2021-11-26T09:11:34.792271Z","shell.execute_reply":"2021-11-26T09:11:34.83501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Clustering Config","metadata":{}},{"cell_type":"code","source":"## :: dataset config :: ##\n\nmaster_id_col = 'ID'                               # already present!, mention the col name\nmaster_clean_text_col = 'clean_text'                      # already present!, mention the col name\nmaster_dup_id_col = \"dup_idx\"                             # already present!, mention the col name\nmaster_similar_id_col = \"similar_idx\"                     # already present!, mention the col name\nmaster_dup_similar_id_col = \"dup_similar_idx\"             # already present!, mention the col name\nmaster_freq_col = \"FREQ\"                                  # already present!, mention the col name\nmaster_coverage_col = \"COVERAGE\"                          # already present!, mention the col name\nmaster_clustering_col = 'FINAL_CLUSTER_ID'\n\n# :: clustering config ::\nKMEANS_RATE = 5\nCLUSTER_K = 0\nKMEANS_SEED_INIT = 50\nKMEANS_MAXITER = 1000\nCLUSTER_LEN = 20\nCOHESION_THRESHOLD = 0.60\nCLUSTER_LEN_2nd_LEVEL = 100\nCLUSTER_LEN_threshold = 2000\nOUTPUT_DUP = \"yes\"","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:11:34.837488Z","iopub.execute_input":"2021-11-26T09:11:34.83782Z","iopub.status.idle":"2021-11-26T09:11:34.85147Z","shell.execute_reply.started":"2021-11-26T09:11:34.837784Z","shell.execute_reply":"2021-11-26T09:11:34.850645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:11:34.852559Z","iopub.execute_input":"2021-11-26T09:11:34.853365Z","iopub.status.idle":"2021-11-26T09:11:34.884052Z","shell.execute_reply.started":"2021-11-26T09:11:34.853309Z","shell.execute_reply":"2021-11-26T09:11:34.883374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Clustering","metadata":{}},{"cell_type":"code","source":"## clustering\n\n# clustering df ---> df without duplicates, or very similar records\ndf[master_dup_similar_id_col] = df[master_dup_similar_id_col].apply(lambda x: tuple(x))\ndf_cluster = df.drop_duplicates(subset=[master_dup_similar_id_col]).reset_index(drop=True)\nprint('\\nTotal Datapoints :: df.shape = ', df.shape)\nprint('\\nDatapoitns with contextually similar records :: df_cluster.shape = ', df_cluster.shape)\n\n# EXECUTE CLUSTERING\nclustered_output = run_clustering(df_cluster, spacy_model=nlp, preprocess=False, vectorizer='bert')","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:11:34.887267Z","iopub.execute_input":"2021-11-26T09:11:34.887687Z","iopub.status.idle":"2021-11-26T09:11:50.033659Z","shell.execute_reply.started":"2021-11-26T09:11:34.88764Z","shell.execute_reply":"2021-11-26T09:11:50.032754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Output","metadata":{}},{"cell_type":"code","source":"### :: FINAL DATAFRAME :: ###\n\n# 1. MERGE 'clustered_output' WITH 'df_cluster' on UNIQUE_ID TO CREATE a 'temp'\ntemp = df_cluster[[master_id_col, master_dup_similar_id_col]].merge(clustered_output[[master_id_col, master_clustering_col,'Seed_Q1','Seed_Q2','Dist_to_Center','Cohesion']], on = master_id_col, how = 'left')\n\n# 2. MERGE 'temp' WITH ORIGNAL DATA (without any drops) 'new_df'\ndf = df.merge(temp[[master_dup_similar_id_col,  master_clustering_col,'Seed_Q1', 'Seed_Q2','Dist_to_Center','Cohesion']], on = master_dup_similar_id_col,how = 'left')\n\n# 3. GET INSIGHTS (Frequency, Coverage)\ndf[master_clustering_col] = df[master_clustering_col].fillna(-1).astype(int)\ncid_count = Counter(df[master_clustering_col])\ndf[master_freq_col] = df[master_clustering_col].apply(lambda cid: cid_count.get(cid, 0))\ndf[master_coverage_col] = df[master_freq_col].apply(lambda x: int(x)*100.0/sum(cid_count.values()))\ndf = df.drop(columns=['cluster_id'])\n\nprint(\"Clusters generated:\", len(cid_count))\nprint(\"Process finished!\")","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:11:50.035394Z","iopub.execute_input":"2021-11-26T09:11:50.035749Z","iopub.status.idle":"2021-11-26T09:11:50.064763Z","shell.execute_reply.started":"2021-11-26T09:11:50.035704Z","shell.execute_reply":"2021-11-26T09:11:50.063893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv(\"df_parent_clusteredResults_26112021.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:12:45.878217Z","iopub.execute_input":"2021-11-26T09:12:45.878588Z","iopub.status.idle":"2021-11-26T09:12:45.924546Z","shell.execute_reply.started":"2021-11-26T09:12:45.878548Z","shell.execute_reply":"2021-11-26T09:12:45.923545Z"},"trusted":true},"execution_count":null,"outputs":[]}]}